{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16964,"status":"ok","timestamp":1669662679696,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"WnuTLgKkQkCh","outputId":"8beb2949-9d6c-479c-898d-a9153a560445"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Esta diseñado para colab, de modo que se pueden descargar y cargar archivos de Drive\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9389,"status":"ok","timestamp":1669662694454,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"6R7Q5sTeP2Iw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90c3f030-4f0a-4187-925b-1258a04584a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/UNI/COLABS\n"]}],"source":["%cd \"/content/drive/My Drive/UNI/COLABS/\"\n","\n","# importar todas las librerias necesarias\n","\n","from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n","from keras.layers import Activation, Dense, Dropout, Flatten, InputLayer\n","from tensorflow.keras.layers import BatchNormalization\n","from keras.callbacks import TensorBoard\n","from keras.models import Sequential, model_from_json\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n","from skimage.color import rgb2lab, lab2rgb, rgb2gray\n","from skimage.io import imsave\n","import numpy as np\n","import os\n","import random\n","import tensorflow as tf\n","\n","from sklearn.metrics import mean_squared_error\n","from PIL import Image, ImageOps\n","import matplotlib.pyplot as plt\n","\n","from colorization.colorizers import *\n","from statistics import mean"]},{"cell_type":"code","source":["%cd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1Zux-WV8-3F","executionInfo":{"status":"ok","timestamp":1669684531234,"user_tz":480,"elapsed":5,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"224d2683-39db-4ee7-f15f-3dd22a60e310"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["X = [] # Crear archivo donde esten todas las imágenes como matrices"],"metadata":{"id":"XdfwEXrm8mi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rko156uKQoIx"},"outputs":[],"source":["for filename in os.listdir('/content/drive/My Drive/UNI/DATA/faces_data_1/color'): # Por cada imagen de color\n","  X.append(img_to_array(load_img('/content/drive/My Drive/UNI/DATA/faces_data_1/color/'+filename))) # Se agrega como matriz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1669662715727,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"aKG-maDFyHNl","outputId":"869cc4b8-1252-4f1c-ed3a-88d29cd31ee5"},"outputs":[{"output_type":"stream","name":"stdout","text":["637\n"]}],"source":["print(len(X)) # Imprimir cantidad de imagenes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7JzxR1_Qsg3"},"outputs":[],"source":["X = np.array(X, dtype=float) # Guardar como arreglo de numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nM4kr0q_QvWw"},"outputs":[],"source":["split = int(0.95*len(X)) # Dividir el 95% para entrenamiento y 5% para pruebas\n","Xtrain = X[:split] # Todas hasta split\n","Xtrain = 1.0/255*Xtrain # Normalizar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1669662716646,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"9ozgK3ISHYe7","outputId":"2f5b710f-64f3-4b0d-f11c-05d6ac257c6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["(605, 256, 256, 3)\n"]}],"source":["print(Xtrain.shape) # 605 imagenes de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pqzj1u-KHeZN"},"outputs":[],"source":["model = Sequential() # Instanciar modelo de keras secuencial para que se sumen los modelos\n","model.add(InputLayer(input_shape=(256, 256, 1))) # Ajustar la entrada para aceptar una matriz de 256 * 256 pixeles\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) # Todos los bloques convolucionales\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.compile(optimizer='rmsprop', loss='mse') # Compilar el modelo y establecer las métricas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0HwT-3zHiUm"},"outputs":[],"source":["datagen = ImageDataGenerator(\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        rotation_range=20,\n","        horizontal_flip=True)\n","\n","# Instancia de ImageDataGenerator que genera grupos de tensor de imágenes con aumentación de datos\n","\n","batch_size = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_5fah4eHnDO"},"outputs":[],"source":["def image_a_b_gen(batch_size):\n","    for batch in datagen.flow(Xtrain, batch_size=batch_size): # Generar los batches\n","        lab_batch = rgb2lab(batch) # Pasar de rgb a lab\n","        X_batch = lab_batch[:,:,:,0] # Separar l\n","        Y_batch = lab_batch[:,:,:,1:] / 128 # Separar ab\n","        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch) # Reconfigurar la matriz para tener el tamaño de la entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohfK9W8EHoVq","executionInfo":{"status":"ok","timestamp":1669530324310,"user_tz":480,"elapsed":2232472,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"c9e8360c-ab39-4039-99a9-0cf520ec273f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","950/950 [==============================] - 233s 233ms/step - loss: 0.0439\n","Epoch 2/10\n","950/950 [==============================] - 221s 233ms/step - loss: 0.0014\n","Epoch 3/10\n","950/950 [==============================] - 221s 233ms/step - loss: 0.0014\n","Epoch 4/10\n","950/950 [==============================] - 221s 233ms/step - loss: 0.0012\n","Epoch 5/10\n","950/950 [==============================] - 223s 235ms/step - loss: 0.0011\n","Epoch 6/10\n","950/950 [==============================] - 222s 234ms/step - loss: 0.0010\n","Epoch 7/10\n","950/950 [==============================] - 222s 234ms/step - loss: 9.1339e-04\n","Epoch 8/10\n","950/950 [==============================] - 222s 234ms/step - loss: 8.3868e-04\n","Epoch 9/10\n","950/950 [==============================] - 222s 234ms/step - loss: 7.6614e-04\n","Epoch 10/10\n","950/950 [==============================] - 222s 234ms/step - loss: 7.0953e-04\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcf5600e390>"]},"metadata":{},"execution_count":12}],"source":["tensorboard = TensorBoard(log_dir=\"/content/drive/My Drive/UNI/AM-PROY-2/Full-version/first_run\") # Instanciar TensorBoard para visualización\n","model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=10, steps_per_epoch=950) # Entrenar el modelo con los batches de imágenes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHBFd9pHBusf"},"outputs":[],"source":["# Si se entrenó, se corre esta celda para guardar el modelo\n","\n","model_json = model.to_json() # json guarda la estructura de los bloques convolusionales\n","with open(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model2.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","model.save_weights(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model_2.h5\") # Guarda los pesos entrenados de las neuronas"]},{"cell_type":"code","source":["# Si se quiere recuperar el modelo sin entrenar, se corre esta celda\n","\n","json_file = open(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model2.json\", 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model = model_from_json(loaded_model_json) # Cargar la estructura de los bloques convolucionales\n","\n","model.load_weights(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model_2.h5\") # Cargar los pesos del modelo\n","\n","model.compile(optimizer='rmsprop', loss='mse') # Compilar el modelo"],"metadata":{"id":"ovPtF5FyoObQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP7hn_xsB3yS"},"outputs":[],"source":["Xtest = rgb2lab(1.0/255*X[split:])[:,:,:,0] # Imagenes de prueba en lab separados solo l\n","Xtest = Xtest.reshape(Xtest.shape+(1,)) # Reconfigurar para que tenga el mismo tamaño que la entrada de la red neuronal\n","Ytest = rgb2lab(1.0/255*X[split:])[:,:,:,1:] # Imagenes de prueba en lab, solamente ab\n","Ytest = Ytest / 128 # Dividir entre 128 para normalizar y que sea a y b de 0 a 1."]},{"cell_type":"code","source":["print(len(Xtest))\n","print(len(Ytest))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VmzGZ2BeCmL","executionInfo":{"status":"ok","timestamp":1669662820732,"user_tz":480,"elapsed":4,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"c5316ddd-c3e2-4f4c-ffcd-089a34cede7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["32\n","32\n"]}]},{"cell_type":"code","source":["batch_size = 10 # MSE del modelo en Keras\n","\n","print(model.evaluate(Xtest, Ytest, batch_size=batch_size))\n","print(model.evaluate(Xtest, Ytest, batch_size = 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irNe_p3netZF","executionInfo":{"status":"ok","timestamp":1669662836909,"user_tz":480,"elapsed":5685,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"d0c849ca-38de-46ef-e44b-7e4411928086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 3s 521ms/step - loss: 7.6007e-04\n","0.0007600698154419661\n","32/32 [==============================] - 3s 88ms/step - loss: 7.6007e-04\n","0.0007600696408189833\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClgHmZAHCCbx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669662862360,"user_tz":480,"elapsed":980,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"9fa6db96-9e98-4ff4-f75f-7dcc65f6af0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["32\n"]}],"source":["#color_me = []\n","#for filename in os.listdir('/content/drive/My Drive/UNI/DATA/faces_data_1/gray'):\n"," # color_me.append(img_to_array(load_img('/content/drive/My Drive/UNI/DATA/faces_data_1/gray/'+filename)))\n","\n","color_me = X[split:] # Reescribir para que sean las mismas que pruebas\n","print(len(color_me))\n","color_me = np.array(color_me, dtype=float) # Modificar a flotantes por si hay enteros\n","color_me = rgb2lab(1.0/255*color_me)[:,:,:,0] # Cambiar a lab, pero solo obtener l\n","color_me = color_me.reshape(color_me.shape+(1,)) # Reconfigurar para que tenga el tamaño de la entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUfSdBfxCcwM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669662866650,"user_tz":480,"elapsed":2784,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"91f8cb28-d4ac-4bb8-ffb7-6c124f1fe94c"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 3s 3s/step\n"]}],"source":["output = model.predict(color_me) # Predecir por cada imágen en el arreglo\n","output = output * 128 # Multiplicar por 128 para regresar de la normalización"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRwGSjHFCga5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669662876890,"user_tz":480,"elapsed":5960,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"48238987-7629-47ce-8918-e2584e0f3317"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"]}],"source":["for i in range(len(output)): # Por cada imágen que regrese el modelo\n","  cur = np.zeros((256, 256, 3)) # Arreglo vacio para juntar la entrada con el output del modelo\n","  cur[:,:,0] = color_me[i][:,:,0]\n","  cur[:,:,1:] = output[i]\n","  imsave(f\"/content/drive/My Drive/UNI/COLABS/pred_3/Testing_{i}.png\", lab2rgb(cur)) # Guardar como imagen el resultado"]},{"cell_type":"code","source":["%cd /content/drive/My Drive/UNI/COLABS/ # Cambiar la ruta\n","\n","colorizer_siggraph17 = siggraph17(pretrained=True).eval() # Cargar el modelo entrenado"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["6d284c0a51f343a38d5655c08450d648","8d2d1d33987747e3a52a57aeb4b72fad","ceca2b3fda624317a5376afd0d650a6e","ad6e113791af41539815c5dd54591003","55ac148f78c24d44859f9c8e3fcfcbd7","452d7350f9464c63ac79ddd637e3355c","97b15eed40ad4ba8986584f0f6953fb5","4deb8bdec58a4433a568f23a896dece8","46fddb5e58ba4c72b6e4e291f6a0651d","60fd53b2a4794fada394986896371ca8","08789b9ce3fe4545939557d723229916"]},"id":"quQg8v4-kAQD","executionInfo":{"status":"ok","timestamp":1669662884795,"user_tz":480,"elapsed":3209,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"1230b1c1-71b9-46ac-caa4-54ee2862895d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/UNI/COLABS\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://colorizers.s3.us-east-2.amazonaws.com/siggraph17-df00044c.pth\" to /root/.cache/torch/hub/checkpoints/siggraph17-df00044c.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/130M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d284c0a51f343a38d5655c08450d648"}},"metadata":{}}]},{"cell_type":"code","source":["Y = np.zeros((403, 256, 256, 3))  # Arreglo vacio que va a guardar las imágenes de prueba\n","\n","for i in range(len(X[split:])): # Por cada imagen de prueba\n","  x = X[split:][i] # Guardar la imagen\n","  img = array_to_img(x) # Transformarla a matriz\n","  img.save(f'original_3/Testing_{i}.png') # Guardar la imagen original\n","  img = ImageOps.grayscale(img) # Transformarla a escala de grises\n","  img.save(f'bw_3/Testing_{i}.png') # Guardar la imágen en escala de grises\n","  img = load_img(f'bw_3/Testing_{i}.png') # Cargar la imagen\n","\n","  # Pasarla por el colorizador siggraph\n","  (tens_l_orig, tens_l_rs) = preprocess_img(img, HW=(256,256))\n","  img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n","  out_img_siggraph17 = postprocess_tens(tens_l_orig, colorizer_siggraph17(tens_l_rs).cpu())\n","  # out_img_siggraph17 matriz que predice el modelo de siggraph\n","  \n","  plt.imsave(f'siggraph_3/Testing_{i}.png', out_img_siggraph17) # Guardar la predicción como imagen\n","  Y[i] = out_img_siggraph17 # Guardar el resultado como matriz"],"metadata":{"id":"XkPrjWnYjrSQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rgb_ab( tup ):\n","  if type(tup) == int: # En caso de que la función regrese getpixel regrese un \n","                       # solo número, se transforma en tupla rgb con ese mismo \n","                       # valor en los 3\n","    tup = (tup, tup, tup)\n","  # Función que transforma rgb a ab con las fórmulas del paper\n","  # Acepta una tupla con 3 valores rgb y regresa una tupla de 2 (a,b)\n","  I = (tup[0] + tup[1] + tup[2])/3\n","  if (I == 0):\n","    return (0,0)\n","  a = (tup[2]/I) - ((tup[0] + tup[1])/(2*I))\n","  b = (tup[0] - tup[1])/I\n","  return (a,b)"],"metadata":{"id":"MEHDzvWIP8nu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z77tFdBNmWAO","executionInfo":{"status":"ok","timestamp":1669662943298,"user_tz":480,"elapsed":6,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"3bdcd287-a5ee-4964-f7db-a1e02184e22a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["# importar nombres de las imágenes de cada directorio\n","images_bw = os.listdir('/content/drive/My Drive/UNI/COLABS/bw_3')\n","images_sig = os.listdir('/content/drive/My Drive/UNI/COLABS/siggraph_3')\n","images_color = os.listdir('/content/drive/My Drive/UNI/COLABS/original_3')\n","images_pred = os.listdir('/content/drive/My Drive/UNI/COLABS/pred_3')\n","\n","# Arreglo que contendrá el MSE de cada imágen\n","MSE_a = []\n","MSE_b = []\n","\n","for i in range(len(images_color)): # Por cada imagen\n","  img_color = Image.open('/content/drive/My Drive/UNI/COLABS/original_3/' + images_color[i]) # Abrir imagen a color\n","  img_pred = Image.open('/content/drive/My Drive/UNI/COLABS/pred_3/' + images_pred[i]) # Abrir predicción\n","\n","  # Se guardan los valores de los pixeles \n","  Y_pred_a = []\n","  Y_pred_b = []\n","  Y_true_a = []\n","  Y_true_b = []\n","\n","  for w in range(img_color.size[0]):\n","    for h in range(img_color.size[1]):\n","      # Por cada pixel en altura y ancho de la imagen\n","        # Regresar el valor a,b del pixel y agregarlo a su respectiva lista entre las imagenes original y la predicción:\n","        Y_pred_a.append(rgb_ab(img_pred.getpixel((w,h)))[0])\n","        Y_pred_b.append(rgb_ab(img_pred.getpixel((w,h)))[1])  \n","        Y_true_a.append(rgb_ab(img_color.getpixel((w,h)))[0])\n","        Y_true_b.append(rgb_ab(img_color.getpixel((w,h)))[1])\n","  \n","  # Guardar el valor final de MSE de toda la imagen en la respectiva lista\n","  MSE_a.append(mean_squared_error(Y_true_a,Y_pred_a))\n","  MSE_b.append(mean_squared_error(Y_true_b,Y_pred_b))\n","\n","# Sacar el promedio de las listas\n","print(\"MSE promedio por imagen entre color y nuestras predicciones\")\n","print(\"a\", mean(MSE_a))\n","print(\"b\", mean(MSE_b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iTq1aMtNhIR","executionInfo":{"status":"ok","timestamp":1669662961977,"user_tz":480,"elapsed":18683,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"b6a11fb0-6c8a-4590-abb0-0f84fdc14190"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE promedio por imagen entre color y nuestras predicciones\n","a 0.019052183358447612\n","b 0.012548303974662549\n"]}]},{"cell_type":"code","source":["# Arreglo que contendrá el MSE de cada imágen\n","MSE_a = []\n","MSE_b = []\n","\n","for i in range(len(images_color)): # Por cada imagen\n","  img_color = Image.open('/content/drive/My Drive/UNI/COLABS/original_3/' + images_color[i]) # Abrir imagen a color\n","  img_sigg = Image.open('/content/drive/My Drive/UNI/COLABS/siggraph_3/' + images_pred[i]) # Abrir predicción\n","\n","  # Se guardan los valores de los pixeles \n","  Y_pred_a = []\n","  Y_pred_b = []\n","  Y_true_a = []\n","  Y_true_b = []\n","\n","  for w in range(img_color.size[0]):\n","    for h in range(img_color.size[1]):\n","      # Por cada pixel en altura y ancho de la imagen\n","        # Regresar el valor a,b del pixel y agregarlo a su respectiva lista entre las imagenes original y la predicción:\n","        Y_pred_a.append(rgb_ab(img_sigg.getpixel((w,h)))[0])\n","        Y_pred_b.append(rgb_ab(img_sigg.getpixel((w,h)))[1])  \n","        Y_true_a.append(rgb_ab(img_color.getpixel((w,h)))[0])\n","        Y_true_b.append(rgb_ab(img_color.getpixel((w,h)))[1])\n","  \n","  # Guardar el valor final de MSE de toda la imagen en la respectiva lista\n","  MSE_a.append(mean_squared_error(Y_true_a,Y_pred_a))\n","  MSE_b.append(mean_squared_error(Y_true_b,Y_pred_b))\n","\n","# Sacar el promedio de las listas\n","print(\"MSE promedio por imagen entre color y algoritmo siggraph\")\n","print(\"a\", mean(MSE_a))\n","print(\"b\", mean(MSE_b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1Nxh9pQQQiT","executionInfo":{"status":"ok","timestamp":1669662980930,"user_tz":480,"elapsed":18954,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"fe155fe9-7200-4e5d-b761-64d04cff3c23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE promedio por imagen entre color y algoritmo siggraph\n","a 0.07071355575768484\n","b 0.060184088661599223\n"]}]},{"cell_type":"code","source":["# Arreglo que contendrá el MSE de cada imágen\n","MSE_a = []\n","MSE_b = []\n","\n","for i in range(len(images_color)): # Por cada imagen\n","  img_color = Image.open('/content/drive/My Drive/UNI/COLABS/original_3/' + images_color[i]) # Abrir imagen a color\n","  img_bw = Image.open('/content/drive/My Drive/UNI/COLABS/bw_3/' + images_pred[i]) # Abrir predicción\n","\n","  # Se guardan los valores de los pixeles \n","  Y_pred_a = []\n","  Y_pred_b = []\n","  Y_true_a = []\n","  Y_true_b = []\n","\n","  for w in range(img_color.size[0]):\n","    for h in range(img_color.size[1]):\n","      # Por cada pixel en altura y ancho de la imagen\n","        # Regresar el valor a,b del pixel y agregarlo a su respectiva lista entre las imagenes original y la predicción:\n","        Y_pred_a.append(rgb_ab(img_bw.getpixel((w,h)))[0])\n","        Y_pred_b.append(rgb_ab(img_bw.getpixel((w,h)))[1])  \n","        Y_true_a.append(rgb_ab(img_color.getpixel((w,h)))[0])\n","        Y_true_b.append(rgb_ab(img_color.getpixel((w,h)))[1])\n","  \n","  # Guardar el valor final de MSE de toda la imagen en la respectiva lista\n","  MSE_a.append(mean_squared_error(Y_true_a,Y_pred_a))\n","  MSE_b.append(mean_squared_error(Y_true_b,Y_pred_b))\n","\n","# Sacar el promedio de las listas\n","print(\"MSE promedio por imagen entre color y escala de grises\")\n","print(\"a\", mean(MSE_a))\n","print(\"b\", mean(MSE_b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OGxMklmRZHR","executionInfo":{"status":"ok","timestamp":1669662999566,"user_tz":480,"elapsed":18641,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"d204d3b0-597d-4c55-b418-254b283551ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE promedio por imagen entre color y escala de grises\n","a 0.16026416314016442\n","b 0.21572652829997022\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMQc6w1nFh3kdUrZqh4iV2c"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6d284c0a51f343a38d5655c08450d648":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d2d1d33987747e3a52a57aeb4b72fad","IPY_MODEL_ceca2b3fda624317a5376afd0d650a6e","IPY_MODEL_ad6e113791af41539815c5dd54591003"],"layout":"IPY_MODEL_55ac148f78c24d44859f9c8e3fcfcbd7"}},"8d2d1d33987747e3a52a57aeb4b72fad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_452d7350f9464c63ac79ddd637e3355c","placeholder":"​","style":"IPY_MODEL_97b15eed40ad4ba8986584f0f6953fb5","value":"100%"}},"ceca2b3fda624317a5376afd0d650a6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4deb8bdec58a4433a568f23a896dece8","max":136787426,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46fddb5e58ba4c72b6e4e291f6a0651d","value":136787426}},"ad6e113791af41539815c5dd54591003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60fd53b2a4794fada394986896371ca8","placeholder":"​","style":"IPY_MODEL_08789b9ce3fe4545939557d723229916","value":" 130M/130M [00:02&lt;00:00, 53.4MB/s]"}},"55ac148f78c24d44859f9c8e3fcfcbd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"452d7350f9464c63ac79ddd637e3355c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97b15eed40ad4ba8986584f0f6953fb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4deb8bdec58a4433a568f23a896dece8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46fddb5e58ba4c72b6e4e291f6a0651d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60fd53b2a4794fada394986896371ca8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08789b9ce3fe4545939557d723229916":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}