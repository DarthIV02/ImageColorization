{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30515,"status":"ok","timestamp":1669682805224,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"WnuTLgKkQkCh","outputId":"a6fea948-0a7c-4ccf-a834-b83661b3fe24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Este notebook fue creado en colab, por lo tanto esta optimizado para que acceda a los archivos de Drive\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"6R7Q5sTeP2Iw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669682859128,"user_tz":480,"elapsed":5497,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"d66903b4-279a-4728-cc5e-f78b8972e688"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/UNI/COLABS\n"]}],"source":["%cd \"/content/drive/My Drive/UNI/COLABS/\"\n","\n","# importar todas las librerias necesarias\n","\n","from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n","from keras.layers import Activation, Dense, Dropout, Flatten, InputLayer\n","from tensorflow.keras.layers import BatchNormalization\n","from keras.callbacks import TensorBoard\n","from keras.models import Sequential, model_from_json\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import array_to_img, img_to_array, load_img\n","from skimage.color import rgb2lab, lab2rgb, rgb2gray\n","from skimage.io import imsave\n","import numpy as np\n","import os\n","import random\n","import tensorflow as tf\n","\n","from sklearn.metrics import mean_squared_error\n","from PIL import Image, ImageOps\n","import matplotlib.pyplot as plt\n","\n","from colorization.colorizers import *\n","from statistics import mean"]},{"cell_type":"code","source":["%cd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6lvqarp2E_a","executionInfo":{"status":"ok","timestamp":1669682725594,"user_tz":480,"elapsed":5,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"aaf3b4e3-f4c2-432b-8abe-eb7c13924798"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["X = [] # Crear archivo donde esten todas las imágenes como matrices"],"metadata":{"id":"XdfwEXrm8mi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rko156uKQoIx"},"outputs":[],"source":["for filename in os.listdir('/content/drive/My Drive/UNI/DATA/natural_color/color'): # Por cada imagen de color\n","  X.append(img_to_array(load_img('/content/drive/My Drive/UNI/DATA/natural_color/color/'+filename))) # Se agrega como matriz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1669621407382,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"aKG-maDFyHNl","outputId":"3f7e406c-6f97-4688-cf5f-6edcda3ee816"},"outputs":[{"output_type":"stream","name":"stdout","text":["688\n"]}],"source":["print(len(X)) # Imprimir cantidad de imagenes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7JzxR1_Qsg3"},"outputs":[],"source":["X = np.array(X, dtype=float) # Guardar como arreglo de numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nM4kr0q_QvWw"},"outputs":[],"source":["split = int(0.95*len(X)) # Dividir el 95% para entrenamiento y 5% para pruebas\n","Xtrain = X[:split] # Todas hasta split\n","Xtrain = 1.0/255*Xtrain # Normalizar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669621408161,"user":{"displayName":"Ximena González","userId":"01616957970933617687"},"user_tz":480},"id":"9ozgK3ISHYe7","outputId":"5a9673e2-7bab-41dd-9fd8-4f4c8a9f716f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(653, 256, 256, 3)\n"]}],"source":["print(Xtrain.shape) # 653 imagenes de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pqzj1u-KHeZN"},"outputs":[],"source":["model = Sequential() # Instanciar modelo de keras secuencial para que se sumen los modelos\n","model.add(InputLayer(input_shape=(256, 256, 1))) # Ajustar la entrada para aceptar una matriz de 256 * 256 pixeles\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) # Todos los bloques convolucionales\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n","model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n","model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n","model.add(UpSampling2D((2, 2)))\n","model.compile(optimizer='rmsprop', loss='mse') # Compilar el modelo y establecer las métricas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0HwT-3zHiUm"},"outputs":[],"source":["datagen = ImageDataGenerator(\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        rotation_range=20,\n","        horizontal_flip=True)\n","\n","# Instancia de ImageDataGenerator que genera grupos de tensor de imágenes con aumentación de datos\n","\n","batch_size = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_5fah4eHnDO"},"outputs":[],"source":["def image_a_b_gen(batch_size):  \n","    for batch in datagen.flow(Xtrain, batch_size=batch_size): # Generar los batches\n","        lab_batch = rgb2lab(batch) # Pasar de rgb a lab\n","        X_batch = lab_batch[:,:,:,0] # Separar l\n","        Y_batch = lab_batch[:,:,:,1:] / 128 # Separar ab\n","        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch) # Reconfigurar la matriz para tener el tamaño de la entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohfK9W8EHoVq","executionInfo":{"status":"ok","timestamp":1669542023341,"user_tz":480,"elapsed":2152596,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"e8abbb4b-124b-4859-f2c1-9072494bad3f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","950/950 [==============================] - 227s 227ms/step - loss: 0.5996\n","Epoch 2/10\n","950/950 [==============================] - 216s 227ms/step - loss: 0.0220\n","Epoch 3/10\n","950/950 [==============================] - 216s 227ms/step - loss: 0.0146\n","Epoch 4/10\n","950/950 [==============================] - 215s 226ms/step - loss: 0.0123\n","Epoch 5/10\n","950/950 [==============================] - 215s 226ms/step - loss: 0.0105\n","Epoch 6/10\n","950/950 [==============================] - 214s 226ms/step - loss: 0.0094\n","Epoch 7/10\n","950/950 [==============================] - 215s 227ms/step - loss: 0.0088\n","Epoch 8/10\n","950/950 [==============================] - 214s 225ms/step - loss: 0.0078\n","Epoch 9/10\n","950/950 [==============================] - 212s 223ms/step - loss: 0.0073\n","Epoch 10/10\n","950/950 [==============================] - 207s 218ms/step - loss: 0.0064\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7308af2750>"]},"metadata":{},"execution_count":19}],"source":["tensorboard = TensorBoard(log_dir=\"/content/drive/My Drive/UNI/AM-PROY-2/Full-version/first_run\") # Instanciar TensorBoard para visualización\n","model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=10, steps_per_epoch=950) # Entrenar el modelo con los batches de imágenes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHBFd9pHBusf"},"outputs":[],"source":["# Si se entrenó, se corre esta celda para guardar el modelo\n","\n","model_json = model.to_json() # json guarda la estructura de los bloques convolusionales\n","with open(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model2.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","model.save_weights(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model_3.h5\") # Guarda los pesos entrenados de las neuronas"]},{"cell_type":"code","source":["# Si se quiere recuperar el modelo sin entrenar, se corre esta celda\n","\n","json_file = open(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model2.json\", 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","model = model_from_json(loaded_model_json) # Cargar la estructura de los bloques convolucionales\n","\n","model.load_weights(\"/content/drive/My Drive/UNI/AM-PROY-2/Beta-version/model_3.h5\") # Cargar los pesos del modelo\n","\n","model.compile(optimizer='rmsprop', loss='mse') # Compilar el modelo"],"metadata":{"id":"QEi_7_Hg1a85"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QP7hn_xsB3yS"},"outputs":[],"source":["Xtest = rgb2lab(1.0/255*X[split:])[:,:,:,0] # Imagenes de prueba en lab separados solo l\n","Xtest = Xtest.reshape(Xtest.shape+(1,)) # Reconfigurar para que tenga el mismo tamaño que la entrada de la red neuronal\n","Ytest = rgb2lab(1.0/255*X[split:])[:,:,:,1:] # Imagenes de prueba en lab, solamente ab\n","Ytest = Ytest / 128 # Dividir entre 128 para normalizar y que sea a y b de 0 a 1."]},{"cell_type":"code","source":["print(len(Xtest))\n","print(len(Ytest))\n","type(Xtest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7VmzGZ2BeCmL","executionInfo":{"status":"ok","timestamp":1669621416900,"user_tz":480,"elapsed":5,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"23bfabbd-fb59-4fcf-98da-d6ed46e0fdd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["35\n","35\n"]},{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["batch_size = 10 # MSE del modelo en Keras\n","\n","print(model.evaluate(Xtest, Ytest, batch_size=batch_size))\n","print(model.evaluate(Xtest, Ytest, batch_size = 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irNe_p3netZF","executionInfo":{"status":"ok","timestamp":1669621425626,"user_tz":480,"elapsed":6207,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"97828db6-122a-4c55-bde6-485333301661"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4/4 [==============================] - 3s 620ms/step - loss: 0.0032\n","0.003204912645742297\n","35/35 [==============================] - 3s 90ms/step - loss: 0.0032\n","0.003204912878572941\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ClgHmZAHCCbx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669622011559,"user_tz":480,"elapsed":1224,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"839d2023-c45e-484e-c4e3-e117d70b8427"},"outputs":[{"output_type":"stream","name":"stdout","text":["35\n"]}],"source":["color_me = [] # Arreglo vacío para guardar las matrices de las imágenes\n","for filename in os.listdir('/content/drive/My Drive/UNI/DATA/natural_color/gray'): # Cargar las imágenes\n","  color_me.append(img_to_array(load_img('/content/drive/My Drive/UNI/DATA/natural_color/gray/'+filename)))\n","\n","color_me = X[split:] # Reescribir para que sean las mismas que pruebas\n","print(len(color_me))\n","color_me = np.array(color_me, dtype=float) # Modificar a flotantes por si hay enteros\n","color_me = rgb2lab(1.0/255*color_me)[:,:,:,0] # Cambiar a lab, pero solo obtener l\n","color_me = color_me.reshape(color_me.shape+(1,)) # Reconfigurar para que tenga el tamaño de la entrada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUfSdBfxCcwM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669622020337,"user_tz":480,"elapsed":3237,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"16fe02da-22f7-4206-d40c-927a9bcae368"},"outputs":[{"output_type":"stream","name":"stdout","text":["2/2 [==============================] - 3s 224ms/step\n"]}],"source":["output = model.predict(color_me) # Predecir por cada imágen en el arreglo\n","output = output * 128 # Multiplicar por 128 para regresar de la normalización"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRwGSjHFCga5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669622229634,"user_tz":480,"elapsed":5347,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"44921ece-57ed-47cc-af88-52716b83f43a"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 4 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 19 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 2 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 12 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 9 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 75 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 6 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 1 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n","/usr/local/lib/python3.7/dist-packages/skimage/color/colorconv.py:1109: UserWarning: Color data out of range: Z < 0 in 5 pixels\n","  return xyz2rgb(lab2xyz(lab, illuminant, observer))\n","WARNING:imageio:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"]}],"source":["for i in range(len(output)): # Por cada imágen que regrese el modelo\n","  cur = np.zeros((256, 256, 3)) # Arreglo vacio para juntar la entrada con el output del modelo\n","  cur[:,:,0] = color_me[i][:,:,0]\n","  cur[:,:,1:] = output[i]\n","  imsave(f\"/content/drive/My Drive/UNI/COLABS/pred/Testing_{i}.png\", lab2rgb(cur)) # Guardar como imagen el resultado"]},{"cell_type":"code","source":["%cd /content/drive/My Drive/UNI/COLABS/ # Cambiar la ruta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AGDzeMb5Ujb","executionInfo":{"status":"ok","timestamp":1669621746312,"user_tz":480,"elapsed":298,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"774f83b9-29ce-40ef-de4b-d415453620cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["colorizer_siggraph17 = siggraph17(pretrained=True).eval() # Cargar el modelo entrenado"],"metadata":{"id":"O89_-sGZ5hVh","colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["c75ee4e76d104b13b3d0ae98b6638890","472ecf1f6c494e55aa0fe8739555a40b","3f5bfdbf55594af8a2ca09371baa9d09","33b04a17fd5c4bd69523bfa90605ede5","737ee5c5c22c40298bf85c8386df7e68","9665b1654eac4130821c43b878a058c1","c3754762a8c24b898af8da17108b3d0a","84203aa2f56b422d858d499f3b9423f1","300e6a88fa394c1aac3d2dbcb0d91196","05a094b6c9874ee9b3e5da3375f79255","5e7750a0784c4e439829374c8b2ab92d"]},"executionInfo":{"status":"ok","timestamp":1669682871965,"user_tz":480,"elapsed":3569,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"9d812f14-8b96-478b-e9e9-131a5ba46a2e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://colorizers.s3.us-east-2.amazonaws.com/siggraph17-df00044c.pth\" to /root/.cache/torch/hub/checkpoints/siggraph17-df00044c.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/130M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c75ee4e76d104b13b3d0ae98b6638890"}},"metadata":{}}]},{"cell_type":"code","source":["Y = np.zeros((35, 256, 256, 3)) # Arreglo vacio que va a guardar las imágenes de prueba\n","\n","for i in range(len(X[split:])): # Por cada imagen de prueba\n","  x = X[split:][i] # Guardar la imagen\n","  img = array_to_img(x) # Transformarla a matriz\n","  img.save(f'original/Testing_{i}.png') # Guardar la imagen original\n","  img = ImageOps.grayscale(img) # Transformarla a escala de grises\n","  img.save(f'bw/Testing_{i}.png') # Guardar la imágen en escala de grises\n","  img = load_img(f'bw/Testing_{i}.png') # Cargar la imagen\n","\n","  # Pasarla por el colorizador siggraph\n","  (tens_l_orig, tens_l_rs) = preprocess_img(img, HW=(256,256))\n","  img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n","  out_img_siggraph17 = postprocess_tens(tens_l_orig, colorizer_siggraph17(tens_l_rs).cpu())\n","  # out_img_siggraph17 matriz que predice el modelo de siggraph\n","\n","  plt.imsave(f'algorithm_siggraph/Testing_{i}.png', out_img_siggraph17) # Guardar la predicción como imagen\n","  Y[i] = out_img_siggraph17 # Guardar el resultado como matriz"],"metadata":{"id":"khwKOWBP6wLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rgb_ab( tup ):\n","  if type(tup) == int: # En caso de que la función regrese getpixel regrese un \n","                       # solo número, se transforma en tupla rgb con ese mismo \n","                       # valor en los 3\n","    tup = (tup, tup, tup)\n","  # Función que transforma rgb a ab con las fórmulas del paper\n","  # Acepta una tupla con 3 valores rgb y regresa una tupla de 2 (a,b)\n","  I = (tup[0] + tup[1] + tup[2])/3\n","  if (I == 0):\n","    return (0,0)\n","  a = (tup[2]/I) - ((tup[0] + tup[1])/(2*I))\n","  b = (tup[0] - tup[1])/I\n","  return (a,b)"],"metadata":{"id":"MEHDzvWIP8nu","executionInfo":{"status":"ok","timestamp":1669682948875,"user_tz":480,"elapsed":291,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["%cd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"womPqTAi21X-","executionInfo":{"status":"ok","timestamp":1669683049962,"user_tz":480,"elapsed":191,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"076447ef-23a3-4543-d3aa-208d9a5bcf07"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n"]}]},{"cell_type":"code","source":["# importar nombres de las imágenes de cada directorio\n","images_bw = os.listdir('/content/drive/My Drive/UNI/COLABS/bw')\n","images_sig = os.listdir('/content/drive/My Drive/UNI/COLABS/algorithm_siggraph')\n","images_color = os.listdir('/content/drive/My Drive/UNI/COLABS/original')\n","images_pred = os.listdir('/content/drive/My Drive/UNI/COLABS/pred')\n","\n","# Arreglo que contendrá el MSE de cada imágen\n","MSE_a = []\n","MSE_b = []\n","\n","for i in range(len(images_color)): # Por cada imagen\n","  img_color = Image.open('/content/drive/My Drive/UNI/COLABS/original/' + images_color[i]) # Abrir imagen a color\n","  img_pred = Image.open('/content/drive/My Drive/UNI/COLABS/pred/' + images_pred[i]) # Abrir predicción\n","\n","  # Se guardan los valores de los pixeles \n","  Y_pred_a = []\n","  Y_pred_b = []\n","  Y_true_a = []\n","  Y_true_b = []\n","\n","  for w in range(img_color.size[0]):\n","    for h in range(img_color.size[1]):\n","      # Por cada pixel en altura y ancho de la imagen\n","        # Regresar el valor a,b del pixel y agregarlo a su respectiva lista entre las imagenes original y la predicción:\n","        Y_pred_a.append(rgb_ab(img_pred.getpixel((w,h)))[0])\n","        Y_pred_b.append(rgb_ab(img_pred.getpixel((w,h)))[1])  \n","        Y_true_a.append(rgb_ab(img_color.getpixel((w,h)))[0])\n","        Y_true_b.append(rgb_ab(img_color.getpixel((w,h)))[1])\n","  \n","  # Guardar el valor final de MSE de toda la imagen en la respectiva lista\n","  MSE_a.append(mean_squared_error(Y_true_a,Y_pred_a))\n","  MSE_b.append(mean_squared_error(Y_true_b,Y_pred_b))\n","\n","# Sacar el promedio de las listas\n","print(\"MSE promedio por imagen entre color y nuestras predicciones\")\n","print(\"a\", mean(MSE_a))\n","print(\"b\", mean(MSE_b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4iTq1aMtNhIR","executionInfo":{"status":"ok","timestamp":1669682988690,"user_tz":480,"elapsed":38048,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"f5296209-25d0-434c-be05-87ac8b68d517"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE promedio por imagen entre color y nuestras predicciones\n","a 0.06090106495983092\n","b 0.011591519796284158\n"]}]},{"cell_type":"code","source":["# Arreglo que contendrá el MSE de cada imágen\n","MSE_a = []\n","MSE_b = []\n","\n","for i in range(len(images_color)): # Por cada imagen\n","  img_color = Image.open('/content/drive/My Drive/UNI/COLABS/original/' + images_color[i]) # Abrir imagen a color\n","  img_sigg = Image.open('/content/drive/My Drive/UNI/COLABS/algorithm_siggraph/' + images_pred[i]) # Abrir predicción\n","\n","  # Se guardan los valores de los pixeles \n","  Y_pred_a = []\n","  Y_pred_b = []\n","  Y_true_a = []\n","  Y_true_b = []\n","\n","  for w in range(img_color.size[0]):\n","    for h in range(img_color.size[1]):\n","      # Por cada pixel en altura y ancho de la imagen\n","        # Regresar el valor a,b del pixel y agregarlo a su respectiva lista entre las imagenes original y la predicción:\n","        Y_pred_a.append(rgb_ab(img_sigg.getpixel((w,h)))[0])\n","        Y_pred_b.append(rgb_ab(img_sigg.getpixel((w,h)))[1])  \n","        Y_true_a.append(rgb_ab(img_color.getpixel((w,h)))[0])\n","        Y_true_b.append(rgb_ab(img_color.getpixel((w,h)))[1])\n","  \n","  # Guardar el valor final de MSE de toda la imagen en la respectiva lista\n","  MSE_a.append(mean_squared_error(Y_true_a,Y_pred_a))\n","  MSE_b.append(mean_squared_error(Y_true_b,Y_pred_b))\n","\n","# Sacar el promedio de las listas\n","print(\"MSE promedio por imagen entre color y algoritmo siggraph\")\n","print(\"a\", mean(MSE_a))\n","print(\"b\", mean(MSE_b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1Nxh9pQQQiT","executionInfo":{"status":"ok","timestamp":1669622866359,"user_tz":480,"elapsed":21404,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"152f44cb-b688-46a4-fe4d-37ddc290a28b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE promedio por imagen entre color y algoritmo siggraph\n","a 0.09295181503978404\n","b 0.009430649917240436\n"]}]},{"cell_type":"code","source":["# Arreglo que contendrá el MSE de cada imágen\n","MSE_a = []\n","MSE_b = []\n","\n","for i in range(len(images_color)): # Por cada imagen\n","  img_color = Image.open('/content/drive/My Drive/UNI/COLABS/original/' + images_color[i]) # Abrir imagen a color\n","  img_bw = Image.open('/content/drive/My Drive/UNI/COLABS/bw/' + images_pred[i]) # Abrir predicción\n","\n","  # Se guardan los valores de los pixeles \n","  Y_pred_a = []\n","  Y_pred_b = []\n","  Y_true_a = []\n","  Y_true_b = []\n","\n","  for w in range(img_color.size[0]):\n","    for h in range(img_color.size[1]):\n","      # Por cada pixel en altura y ancho de la imagen\n","        # Regresar el valor a,b del pixel y agregarlo a su respectiva lista entre las imagenes original y la predicción:\n","        Y_pred_a.append(rgb_ab(img_bw.getpixel((w,h)))[0])\n","        Y_pred_b.append(rgb_ab(img_bw.getpixel((w,h)))[1])  \n","        Y_true_a.append(rgb_ab(img_color.getpixel((w,h)))[0])\n","        Y_true_b.append(rgb_ab(img_color.getpixel((w,h)))[1])\n","  \n","  # Guardar el valor final de MSE de toda la imagen en la respectiva lista\n","  MSE_a.append(mean_squared_error(Y_true_a,Y_pred_a))\n","  MSE_b.append(mean_squared_error(Y_true_b,Y_pred_b))\n","\n","# Sacar el promedio de las listas\n","print(\"MSE promedio por imagen entre color y escala de grises\")\n","print(\"a\", mean(MSE_a))\n","print(\"b\", mean(MSE_b))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OGxMklmRZHR","executionInfo":{"status":"ok","timestamp":1669683037337,"user_tz":480,"elapsed":29654,"user":{"displayName":"Ximena González","userId":"01616957970933617687"}},"outputId":"f0063c43-0593-4ef9-89a5-69bf8e26f0e2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE promedio por imagen entre color y escala de grises\n","a 0.29198281505354023\n","b 0.028430336858659706\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP50Le5WezMHFRIuGrs4yHY"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c75ee4e76d104b13b3d0ae98b6638890":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_472ecf1f6c494e55aa0fe8739555a40b","IPY_MODEL_3f5bfdbf55594af8a2ca09371baa9d09","IPY_MODEL_33b04a17fd5c4bd69523bfa90605ede5"],"layout":"IPY_MODEL_737ee5c5c22c40298bf85c8386df7e68"}},"472ecf1f6c494e55aa0fe8739555a40b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9665b1654eac4130821c43b878a058c1","placeholder":"​","style":"IPY_MODEL_c3754762a8c24b898af8da17108b3d0a","value":"100%"}},"3f5bfdbf55594af8a2ca09371baa9d09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_84203aa2f56b422d858d499f3b9423f1","max":136787426,"min":0,"orientation":"horizontal","style":"IPY_MODEL_300e6a88fa394c1aac3d2dbcb0d91196","value":136787426}},"33b04a17fd5c4bd69523bfa90605ede5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05a094b6c9874ee9b3e5da3375f79255","placeholder":"​","style":"IPY_MODEL_5e7750a0784c4e439829374c8b2ab92d","value":" 130M/130M [00:02&lt;00:00, 66.2MB/s]"}},"737ee5c5c22c40298bf85c8386df7e68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9665b1654eac4130821c43b878a058c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3754762a8c24b898af8da17108b3d0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84203aa2f56b422d858d499f3b9423f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"300e6a88fa394c1aac3d2dbcb0d91196":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05a094b6c9874ee9b3e5da3375f79255":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e7750a0784c4e439829374c8b2ab92d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}